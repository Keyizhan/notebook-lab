{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ç«¯åˆ°ç«¯ VQ-VAE è®­ç»ƒï¼šå®Œæ•´ç‰ˆ GCPNet + Transformer + VQ + Decoder\n",
        "\n",
        "æœ¬ notebook å®žçŽ°**å®Œæ•´çš„ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹**ï¼Œé€‚é…å‡çº§åŽçš„ GCPNet æž¶æž„ã€‚\n",
        "\n",
        "## âœ¨ æ›´æ–°å†…å®¹ï¼ˆ2025-12-03ï¼‰\n",
        "\n",
        "### æž¶æž„å‡çº§\n",
        "- **GCPNet ç¼–ç å™¨**ï¼šå®Œæ•´ç‰ˆ 6 å±‚æž¶æž„ï¼ˆ128ç»´æ ‡é‡ + 16ç»´å‘é‡ç‰¹å¾ï¼‰\n",
        "- **ç‰¹å¾æå–å™¨**ï¼šåŒ…å« alphaã€kappaã€dihedrals ç­‰å®Œæ•´å‡ ä½•ç‰¹å¾\n",
        "- **é…ä½“/ç›¸äº’ä½œç”¨ç¼–ç å™¨**ï¼šä»Žç®€å• MLP å‡çº§ä¸º GCPNet æž¶æž„\n",
        "- **ç‰¹å¾ç»´åº¦**ï¼šè¾¹çº§èžåˆç‰¹å¾ä»Ž 257 ç»´å‡çº§ä¸ºæ›´é«˜ç»´åº¦ï¼ˆå–å†³äºŽ GCPNet è¾“å‡ºï¼‰\n",
        "\n",
        "### å…³é”®å·®å¼‚\n",
        "\n",
        "| é¡¹ç›® | æ—§ç‰ˆæœ¬ | æ–°ç‰ˆæœ¬ï¼ˆå‡çº§åŽï¼‰ |\n",
        "|------|--------|-----------------|\n",
        "| **è›‹ç™½ç¼–ç å™¨** | ç®€åŒ–ç‰ˆ GCPNetï¼ˆ39ç»´è¾“å…¥ï¼‰ | å®Œæ•´ç‰ˆ GCPNetï¼ˆ49ç»´è¾“å…¥ï¼‰ |\n",
        "| **é…ä½“ç¼–ç å™¨** | SimpleLigandEncoder MLP | GCPNet æž¶æž„ |\n",
        "| **ç›¸äº’ä½œç”¨ç¼–ç å™¨** | SimpleLigandEncoder MLP | GCPNet æž¶æž„ |\n",
        "| **ç‰¹å¾ç»´åº¦** | 257 ç»´ | 641 ç»´ï¼ˆ128*3 + 257ï¼‰ |\n",
        "| **è®­ç»ƒæ¨¡å¼** | ç«¯åˆ°ç«¯å¯è®­ç»ƒ | ç«¯åˆ°ç«¯å¯è®­ç»ƒ |\n",
        "\n",
        "## æ•°æ®æ¥æº\n",
        "\n",
        "ä½¿ç”¨ `feature extraction/full_pipeline.py` ç”Ÿæˆçš„ HDF5 æ•°æ®ï¼š\n",
        "- `improtant data/binding_edge_features_fused.h5` - è¾¹çº§èžåˆç‰¹å¾ï¼ˆ641 ç»´ï¼‰\n",
        "\n",
        "## è®­ç»ƒæµç¨‹\n",
        "\n",
        "1. åŠ è½½ HDF5 æ•°æ®\n",
        "2. æž„å»ºå®Œæ•´æ¨¡åž‹ï¼šå®Œæ•´ç‰ˆ GCPNet + Featuriser + VQ-VAE\n",
        "3. è®¾ç½®åˆ†å±‚å­¦ä¹ çŽ‡ä¼˜åŒ–å™¨\n",
        "4. ç«¯åˆ°ç«¯è®­ç»ƒï¼ˆæ¢¯åº¦å›žä¼ åˆ° GCPNetï¼‰\n",
        "5. ä¿å­˜ checkpoint\n",
        "6. æŽ¨ç†éªŒè¯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1ï¼šçŽ¯å¢ƒé…ç½®ä¸Žè·¯å¾„è®¾ç½®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 5060 Ti\n",
            "GPU Memory: 17.10 GB\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "import logging\n",
        "\n",
        "# è·¯å¾„é…ç½®\n",
        "BASE_DIR = Path(r'c:/Users/Administrator/Desktop/IGEM/stage1/notebook-lab')\n",
        "sys.path.insert(0, str(BASE_DIR))\n",
        "\n",
        "# æ•°æ®è·¯å¾„\n",
        "H5_DATA_PATH = BASE_DIR / 'improtant data' / 'binding_edge_features_fused.h5'\n",
        "CHECKPOINT_DIR = BASE_DIR / 'checkpoints' / 'vqvae_end_to_end'\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# é…ç½®æ–‡ä»¶\n",
        "VQVAE_CONFIG = BASE_DIR / 'config_vqvae.yaml'\n",
        "GCPNET_CONFIG = BASE_DIR / 'config_gcpnet_encoder.yaml'\n",
        "\n",
        "# è®¾å¤‡\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1.5ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆå¯é€‰ï¼‰\n",
        "\n",
        "å¦‚æžœ HDF5 æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿è¡Œæ­¤ cell è‡ªåŠ¨ç”Ÿæˆæ•°æ®ã€‚\n",
        "\n",
        "**æ³¨æ„**ï¼šæ­¤æ­¥éª¤éœ€è¦ 10-30 åˆ†é’Ÿï¼Œå–å†³äºŽæœºå™¨æ€§èƒ½ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Data Check] Checking for HDF5 data file...\n",
            "  Looking for: c:\\Users\\Administrator\\Desktop\\IGEM\\stage1\\notebook-lab\\improtant data\\binding_edge_features_fused.h5\n",
            "  âœ“ HDF5 data file found!\n",
            "  File size: 33.28 MB\n",
            "  Num edges: 12856\n",
            "  Feature dim: 641\n",
            "\\n  â†’ Skip to Step 2 to load data\n"
          ]
        }
      ],
      "source": [
        "# æ£€æŸ¥ HDF5 æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "print('[Data Check] Checking for HDF5 data file...')\n",
        "print(f'  Looking for: {H5_DATA_PATH}')\n",
        "\n",
        "if H5_DATA_PATH.exists():\n",
        "    print(f'  âœ“ HDF5 data file found!')\n",
        "    print(f'  File size: {H5_DATA_PATH.stat().st_size / 1e6:.2f} MB')\n",
        "    \n",
        "    # å¿«é€Ÿæ£€æŸ¥æ•°æ®\n",
        "    with h5py.File(H5_DATA_PATH, 'r') as f:\n",
        "        num_edges = f.attrs.get('num_edges', 'unknown')\n",
        "        feature_dim = f.attrs.get('feature_dim', 'unknown')\n",
        "        print(f'  Num edges: {num_edges}')\n",
        "        print(f'  Feature dim: {feature_dim}')\n",
        "    \n",
        "    print('\\\\n  â†’ Skip to Step 2 to load data')\n",
        "else:\n",
        "    print(f'  âœ— HDF5 data file not found!')\n",
        "    print(f'\\\\n  â†’ Run the cell below to generate data automatically')\n",
        "    print(f'  â†’ Or manually run: python \"feature extraction/full_pipeline.py\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### è‡ªåŠ¨è¿è¡Œ full_pipeline.py ç”Ÿæˆæ•°æ®\n",
        "\n",
        "**è¿è¡Œæ­¤ cell å°†è‡ªåŠ¨æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹**ï¼š\n",
        "\n",
        "1. åˆ†æž 3432 ä¸ª PDB æ–‡ä»¶ï¼Œè¯†åˆ«è›‹ç™½-é…ä½“æŽ¥è§¦\n",
        "2. æž„å»ºä¸‰å¼ å›¾ï¼ˆè›‹ç™½ã€é…ä½“ã€ç›¸äº’ä½œç”¨ï¼‰å¹¶ç”¨ GCPNet ç¼–ç \n",
        "3. æå–è¾¹çº§å±€éƒ¨ç‰¹å¾\n",
        "4. èžåˆå››ä¸ªæ–‡ä»¶ç”Ÿæˆæœ€ç»ˆçš„ `binding_edge_features_fused.h5`\n",
        "\n",
        "**é¢„è®¡æ—¶é—´**ï¼š10-30 åˆ†é’Ÿ\n",
        "\n",
        "**è¾“å‡ºä½ç½®**ï¼š`improtant data/` ç›®å½•ä¸‹çš„ 6 ä¸ª HDF5 æ–‡ä»¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è‡ªåŠ¨è¿è¡Œ full_pipeline.py ç”Ÿæˆæ•°æ®\n",
        "# åªæœ‰åœ¨æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨æ—¶æ‰è¿è¡Œ\n",
        "\n",
        "if not H5_DATA_PATH.exists():\n",
        "    print('=' * 70)\n",
        "    print('å¼€å§‹è¿è¡Œ full_pipeline.py ç”Ÿæˆ HDF5 æ•°æ®')\n",
        "    print('=' * 70)\n",
        "    print('\\\\nâš  æ­¤è¿‡ç¨‹éœ€è¦ 10-30 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...\\\\n')\n",
        "    \n",
        "    # å¯¼å…¥ full_pipeline æ¨¡å—\n",
        "    pipeline_script = BASE_DIR / 'feature extraction' / 'full_pipeline.py'\n",
        "    \n",
        "    if not pipeline_script.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"æ‰¾ä¸åˆ° full_pipeline.py: {pipeline_script}\\\\n\"\n",
        "            f\"è¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®\"\n",
        "        )\n",
        "    \n",
        "    # æ–¹æ³• 1ï¼šç›´æŽ¥å¯¼å…¥å¹¶è¿è¡Œï¼ˆæŽ¨èï¼‰\n",
        "    try:\n",
        "        import importlib.util\n",
        "        spec = importlib.util.spec_from_file_location(\"full_pipeline\", pipeline_script)\n",
        "        full_pipeline = importlib.util.module_from_spec(spec)\n",
        "        spec.loader.exec_module(full_pipeline)\n",
        "        \n",
        "        # è¿è¡Œä¸»å‡½æ•°\n",
        "        print('\\\\nå¼€å§‹æ‰§è¡Œ full_pipeline.main()...\\\\n')\n",
        "        full_pipeline.main()\n",
        "        \n",
        "        print('\\\\n' + '=' * 70)\n",
        "        print('âœ“ æ•°æ®ç”Ÿæˆå®Œæˆï¼')\n",
        "        print('=' * 70)\n",
        "        \n",
        "        # éªŒè¯æ–‡ä»¶æ˜¯å¦ç”Ÿæˆ\n",
        "        if H5_DATA_PATH.exists():\n",
        "            print(f'\\\\nâœ“ æˆåŠŸç”Ÿæˆ: {H5_DATA_PATH}')\n",
        "            print(f'  File size: {H5_DATA_PATH.stat().st_size / 1e6:.2f} MB')\n",
        "            \n",
        "            with h5py.File(H5_DATA_PATH, 'r') as f:\n",
        "                num_edges = f.attrs.get('num_edges', 'unknown')\n",
        "                feature_dim = f.attrs.get('feature_dim', 'unknown')\n",
        "                print(f'  Num edges: {num_edges}')\n",
        "                print(f'  Feature dim: {feature_dim}')\n",
        "        else:\n",
        "            print('\\\\nâš  è­¦å‘Šï¼šæ•°æ®æ–‡ä»¶æœªç”Ÿæˆï¼Œè¯·æ£€æŸ¥ full_pipeline.py çš„è¾“å‡º')\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f'\\\\nâœ— è¿è¡Œ full_pipeline.py æ—¶å‡ºé”™: {e}')\n",
        "        print('\\\\nè¯·å°è¯•æ‰‹åŠ¨è¿è¡Œï¼š')\n",
        "        print(f'  cd \"{BASE_DIR / \"feature extraction\"}\"')\n",
        "        print(f'  python full_pipeline.py')\n",
        "        raise\n",
        "\n",
        "else:\n",
        "    print('âœ“ HDF5 æ•°æ®æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æ•°æ®ç”Ÿæˆæ­¥éª¤')\n",
        "    print(f'  æ–‡ä»¶è·¯å¾„: {H5_DATA_PATH}')\n",
        "    print('\\\\nâ†’ ç»§ç»­æ‰§è¡Œ Step 2 åŠ è½½æ•°æ®')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2ï¼šæ•°æ®é›†å®šä¹‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Dataset] Loading edge feature dataset...\n",
            "  Loaded 12856 edges from 3139 graphs\n",
            "  Feature dim: 641\n",
            "  Max edges per sample: 512\n",
            "\n",
            "âœ“ Dataset loaded: 3139 samples\n"
          ]
        }
      ],
      "source": [
        "class EdgeFeatureDataset(Dataset):\n",
        "    \"\"\"\n",
        "    ä»Ž HDF5 æ–‡ä»¶åŠ è½½è¾¹çº§èžåˆç‰¹å¾ã€‚\n",
        "    \n",
        "    æ•°æ®æ¥æºï¼šfeature extraction/full_pipeline.py ç”Ÿæˆçš„\n",
        "    improtant data/binding_edge_features_fused.h5\n",
        "    \n",
        "    åŒ…å«ï¼š\n",
        "    - features: (N_edges, feature_dim) èžåˆç‰¹å¾çŸ©é˜µ\n",
        "      * æ—§ç‰ˆæœ¬ï¼š257 ç»´ï¼ˆç®€åŒ– GCPNetï¼‰\n",
        "      * æ–°ç‰ˆæœ¬ï¼š641 ç»´ï¼ˆå®Œæ•´ GCPNetï¼‰\n",
        "    - å…ƒä¿¡æ¯ï¼špdb_id, ligand_resname, graph_index, etc.\n",
        "    \"\"\"\n",
        "    def __init__(self, h5_path, max_edges_per_sample=512):\n",
        "        self.h5_path = h5_path\n",
        "        self.max_edges = max_edges_per_sample\n",
        "         \n",
        "        with h5py.File(h5_path, 'r') as f:\n",
        "            self.features = f['features'][:]\n",
        "            self.graph_indices = f['graph_index'][:]\n",
        "            self.num_edges = len(self.features)\n",
        "            self.num_graphs = int(self.graph_indices.max()) + 1\n",
        "            \n",
        "            # è¯»å–å…ƒä¿¡æ¯\n",
        "            self.pdb_ids = f['pdb_id'][:].astype(str)\n",
        "            self.ligand_resnames = f['ligand_resname'][:].astype(str)\n",
        "            \n",
        "            # è¯»å–å±žæ€§ä¿¡æ¯\n",
        "            self.feature_dim = f.attrs.get('feature_dim', self.features.shape[1])\n",
        "            self.edge_feature_dim = f.attrs.get('edge_feature_dim', 'unknown')\n",
        "            self.protein_emb_dim = f.attrs.get('protein_emb_dim', 'unknown')\n",
        "            self.ligand_emb_dim = f.attrs.get('ligand_emb_dim', 'unknown')\n",
        "            self.interaction_emb_dim = f.attrs.get('interaction_emb_dim', 'unknown')\n",
        "        \n",
        "        print(f'  Loaded {self.num_edges} edges from {self.num_graphs} graphs')\n",
        "        print(f'  Feature dim: {self.features.shape[1]}')\n",
        "        print(f'  Feature composition:')\n",
        "        print(f'    - Edge local features: {self.edge_feature_dim}')\n",
        "        print(f'    - Protein embedding: {self.protein_emb_dim}')\n",
        "        print(f'    - Ligand embedding: {self.ligand_emb_dim}')\n",
        "        print(f'    - Interaction embedding: {self.interaction_emb_dim}')\n",
        "        print(f'  Max edges per sample: {self.max_edges}')\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.num_graphs\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # èŽ·å–å±žäºŽè¯¥å›¾çš„æ‰€æœ‰è¾¹\n",
        "        mask_graph = (self.graph_indices == idx)\n",
        "        graph_features = self.features[mask_graph]\n",
        "        \n",
        "        # æˆªæ–­æˆ–å¡«å……åˆ°å›ºå®šé•¿åº¦\n",
        "        num_edges = len(graph_features)\n",
        "        if num_edges > self.max_edges:\n",
        "            graph_features = graph_features[:self.max_edges]\n",
        "            num_edges = self.max_edges\n",
        "        \n",
        "        # åˆ›å»º maskï¼ˆ1 è¡¨ç¤ºæœ‰æ•ˆè¾¹ï¼Œ0 è¡¨ç¤ºå¡«å……ï¼‰\n",
        "        mask = torch.zeros(self.max_edges, dtype=torch.float32)\n",
        "        mask[:num_edges] = 1.0\n",
        "        \n",
        "        # å¡«å……åˆ°å›ºå®šé•¿åº¦\n",
        "        padded_features = np.zeros((self.max_edges, self.features.shape[1]), dtype=np.float32)\n",
        "        padded_features[:num_edges] = graph_features\n",
        "        \n",
        "        return torch.from_numpy(padded_features), mask\n",
        "\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†\n",
        "print('\\n[Dataset] Loading edge feature dataset...')\n",
        "dataset = EdgeFeatureDataset(H5_DATA_PATH, max_edges_per_sample=512)\n",
        "print(f'\\nâœ“ Dataset loaded: {len(dataset)} samples')\n",
        "print(f'\\nðŸ’¡ ç‰¹å¾ç»´åº¦è¯´æ˜Žï¼š')\n",
        "print(f'  å¦‚æžœæ˜¯ 641 ç»´ â†’ ä½¿ç”¨å®Œæ•´ç‰ˆ GCPNetï¼ˆæŽ¨èï¼‰')\n",
        "print(f'  å¦‚æžœæ˜¯ 257 ç»´ â†’ ä½¿ç”¨ç®€åŒ–ç‰ˆ GCPNetï¼ˆæ—§ç‰ˆæœ¬ï¼‰')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”„ æž¶æž„å‡çº§è¯´æ˜Ž\n",
        "\n",
        "### å®Œæ•´ç‰ˆ GCPNet ç‰¹å¾\n",
        "\n",
        "**æ—§ç‰ˆæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š**\n",
        "- èŠ‚ç‚¹ç‰¹å¾ï¼š`amino_acid_one_hot` (23) + `sequence_positional_encoding` (16) = **39 ç»´**\n",
        "- å‘é‡ç‰¹å¾ï¼š`orientation` (2 ç»´)\n",
        "\n",
        "**æ–°ç‰ˆæœ¬ï¼ˆå®Œæ•´ç‰ˆï¼‰ï¼š**\n",
        "- èŠ‚ç‚¹ç‰¹å¾ï¼š\n",
        "  - `amino_acid_one_hot` (23)\n",
        "  - `sequence_positional_encoding` (16)\n",
        "  - `alpha` (1) - è™šæ‹Ÿæ‰­è½¬è§’\n",
        "  - `kappa` (1) - è™šæ‹Ÿå¼¯æ›²è§’\n",
        "  - `dihedrals` (8) - äºŒé¢è§’ç‰¹å¾\n",
        "  - **æ€»è®¡ï¼š49 ç»´**\n",
        "- å‘é‡ç‰¹å¾ï¼š`orientation` (2 ç»´)\n",
        "\n",
        "### ç¼–ç å™¨æž¶æž„å¯¹æ¯”\n",
        "\n",
        "| ç»„ä»¶ | æ—§ç‰ˆæœ¬ | æ–°ç‰ˆæœ¬ |\n",
        "|------|--------|--------|\n",
        "| **è›‹ç™½ç¼–ç å™¨** | ç®€åŒ– GCPNet (39ç»´è¾“å…¥) | å®Œæ•´ GCPNet (49ç»´è¾“å…¥) |\n",
        "| **é…ä½“ç¼–ç å™¨** | SimpleLigandEncoder (MLP) | GCPNet æž¶æž„ |\n",
        "| **ç›¸äº’ä½œç”¨ç¼–ç å™¨** | SimpleLigandEncoder (MLP) | GCPNet æž¶æž„ |\n",
        "| **è¾“å‡ºç»´åº¦** | 128 ç»´ | 128 ç»´ |\n",
        "\n",
        "### èžåˆç‰¹å¾ç»´åº¦\n",
        "\n",
        "- **è›‹ç™½ embedding**: 128 ç»´ï¼ˆGCPNet è¾“å‡ºï¼‰\n",
        "- **é…ä½“ embedding**: 128 ç»´ï¼ˆGCPNet è¾“å‡ºï¼‰\n",
        "- **ç›¸äº’ä½œç”¨ embedding**: 128 ç»´ï¼ˆGCPNet è¾“å‡ºï¼‰\n",
        "- **è¾¹çº§å±€éƒ¨ç‰¹å¾**: 257 ç»´ï¼ˆèŠ‚ç‚¹ç‰¹å¾ + è·ç¦»ï¼‰\n",
        "- **èžåˆåŽæ€»ç»´åº¦**: 128 + 128 + 128 + 257 = **641 ç»´**\n",
        "\n",
        "### é¢„æœŸæ•ˆæžœ\n",
        "\n",
        "âœ… **æ›´ä¸°å¯Œçš„å‡ ä½•ç‰¹å¾**ï¼šåŒ…å«å®Œæ•´çš„è›‹ç™½è´¨å‡ ä½•ä¿¡æ¯ï¼ˆè§’åº¦ã€æ›²çŽ‡ç­‰ï¼‰  \n",
        "âœ… **ç»Ÿä¸€çš„ç¼–ç æž¶æž„**ï¼šä¸‰ç§å›¾éƒ½ä½¿ç”¨ GCPNetï¼Œè¡¨ç¤ºèƒ½åŠ›æ›´å¼º  \n",
        "âœ… **æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›**ï¼šå®Œæ•´ç‰¹å¾æœ‰åŠ©äºŽæ•èŽ·å¤æ‚çš„è›‹ç™½-é…ä½“ç›¸äº’ä½œç”¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3ï¼šæ¨¡åž‹æž„å»ºï¼ˆç«¯åˆ°ç«¯ç‰ˆæœ¬ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Model] Building end-to-end architecture...\n",
            "\n",
            "[GCPNet] Building GCPNet encoder...\n",
            "  âš  GCPNet checkpoint not found, using random init\n",
            "  GCPNet mode: TRAIN\n",
            "  Featuriser mode: TRAIN\n",
            "\n",
            "[Projector] Feature projector: 641 -> 128 dim\n",
            "\n",
            "[VQ-VAE] Building VQVAETransformer...\n",
            "  âœ“ Using ReconstructionDecoder: 128 -> 641 dim\n",
            "  âš  Disabled TikTok compression to maintain sequence length\n",
            "  âœ“ VQVAETransformer built\n",
            "    - Encoder: 8 layers, 1024 dim\n",
            "    - VQ: 4096 codes, 128 dim\n",
            "    - Residual VQ: enabled (4 layers)\n",
            "    - TikTok compression: disabled (sequence length preserved)\n",
            "    - Total codebook combinations: 281,474,976,710,656 (4096^4)\n",
            "\n",
            "âœ“ ç«¯åˆ°ç«¯æ¨¡åž‹æž„å»ºå®Œæˆï¼\n"
          ]
        }
      ],
      "source": [
        "from vqvae import VQVAETransformer\n",
        "from gcpnet.models.graph_encoders.gcpnet import GCPNetModel\n",
        "from gcpnet.features.factory import ProteinFeaturiser\n",
        "from types import SimpleNamespace\n",
        "\n",
        "print('\\n[Model] Building end-to-end architecture with upgraded GCPNet...')\n",
        "\n",
        "# ------------------ 1. åŠ è½½é…ç½® ------------------\n",
        "vqvae_configs = OmegaConf.load(str(VQVAE_CONFIG))\n",
        "gcpnet_configs = OmegaConf.load(str(GCPNET_CONFIG))\n",
        "\n",
        "# ------------------ 2. æž„å»ºå®Œæ•´ç‰ˆ GCPNetï¼ˆå¯è®­ç»ƒæ¨¡å¼ï¼‰------------------\n",
        "print('\\n[GCPNet] Building full GCPNet encoder...')\n",
        "\n",
        "# å®Œæ•´ç‰ˆ Featuriserï¼ˆåŒ¹é… config_gcpnet_encoder.yamlï¼‰\n",
        "featuriser = ProteinFeaturiser(\n",
        "    representation=\"CA\",\n",
        "    scalar_node_features=[\n",
        "        \"amino_acid_one_hot\",\n",
        "        \"sequence_positional_encoding\",\n",
        "        \"alpha\",\n",
        "        \"kappa\",\n",
        "        \"dihedrals\"\n",
        "    ],\n",
        "    vector_node_features=[\"orientation\"],\n",
        "    edge_types=[\"knn_16\"],\n",
        "    scalar_edge_features=[\"edge_distance\"],\n",
        "    vector_edge_features=[\"edge_vectors\"],\n",
        ").to(device)\n",
        "featuriser.train()  # â† å…³é”®ï¼šè®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼\n",
        "\n",
        "# è½¬æ¢é…ç½®ä¸ºå¯ç”¨æ ¼å¼\n",
        "def dict_to_namespace(d):\n",
        "    \"\"\"é€’å½’åœ°å°†å­—å…¸è½¬æ¢ä¸º SimpleNamespace\"\"\"\n",
        "    if isinstance(d, dict):\n",
        "        return SimpleNamespace(**{k: dict_to_namespace(v) for k, v in d.items()})\n",
        "    elif isinstance(d, list):\n",
        "        return [dict_to_namespace(item) for item in d]\n",
        "    else:\n",
        "        return d\n",
        "\n",
        "enc_kwargs_dict = OmegaConf.to_container(gcpnet_configs.encoder.kwargs, resolve=True)\n",
        "\n",
        "# æž„å»º GCPNet encoder å‚æ•°\n",
        "gcpnet_kwargs = {\n",
        "    'num_layers': enc_kwargs_dict['num_layers'],\n",
        "    'emb_dim': enc_kwargs_dict['emb_dim'],\n",
        "    'node_s_emb_dim': enc_kwargs_dict['node_s_emb_dim'],\n",
        "    'node_v_emb_dim': enc_kwargs_dict['node_v_emb_dim'],\n",
        "    'edge_s_emb_dim': enc_kwargs_dict['edge_s_emb_dim'],\n",
        "    'edge_v_emb_dim': enc_kwargs_dict['edge_v_emb_dim'],\n",
        "    'r_max': enc_kwargs_dict['r_max'],\n",
        "    'num_rbf': enc_kwargs_dict['num_rbf'],\n",
        "    'activation': enc_kwargs_dict['activation'],\n",
        "    'pool': enc_kwargs_dict['pool'],\n",
        "    'module_cfg': dict_to_namespace(enc_kwargs_dict['module_cfg']),\n",
        "    'model_cfg': dict_to_namespace(enc_kwargs_dict['model_cfg']),\n",
        "    'layer_cfg': dict_to_namespace(enc_kwargs_dict['layer_cfg']),\n",
        "}\n",
        "\n",
        "gcpnet_encoder = GCPNetModel(**gcpnet_kwargs).to(device)\n",
        "gcpnet_encoder.train()  # â† å…³é”®ï¼šè®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼\n",
        "\n",
        "print(f'  âœ“ Full GCPNet encoder initialized')\n",
        "print(f'    - Layers: {gcpnet_kwargs[\"num_layers\"]}')\n",
        "print(f'    - Node scalar dim: {gcpnet_kwargs[\"node_s_emb_dim\"]}')\n",
        "print(f'    - Node vector dim: {gcpnet_kwargs[\"node_v_emb_dim\"]}')\n",
        "print(f'    - Input features: amino_acid_one_hot + seq_pos + alpha + kappa + dihedrals')\n",
        "\n",
        "# å¯é€‰ï¼šåŠ è½½é¢„è®­ç»ƒæƒé‡\n",
        "gcpnet_ckpt_path = BASE_DIR / 'models' / 'checkpoints' / 'structure_denoising' / 'gcpnet' / 'ca_bb' / 'last.ckpt'\n",
        "if gcpnet_ckpt_path.exists():\n",
        "    try:\n",
        "        ckpt = torch.load(gcpnet_ckpt_path, map_location=device)\n",
        "        gcpnet_encoder.load_state_dict(ckpt['state_dict'], strict=False)\n",
        "        print('  âœ“ Loaded GCPNet pretrained weights for fine-tuning')\n",
        "    except Exception as e:\n",
        "        print(f'  âš  Failed to load checkpoint: {e}')\n",
        "        print('  â†’ Using random initialization')\n",
        "else:\n",
        "    print('  âš  GCPNet checkpoint not found, using random init')\n",
        "\n",
        "print(f'  GCPNet mode: {\"TRAIN\" if gcpnet_encoder.training else \"EVAL\"}')\n",
        "print(f'  Featuriser mode: {\"TRAIN\" if featuriser.training else \"EVAL\"}')\n",
        "\n",
        "# ------------------ 3. ç‰¹å¾æŠ•å½±å±‚ ------------------\n",
        "# æ£€æŸ¥å®žé™…çš„ç‰¹å¾ç»´åº¦\n",
        "with h5py.File(H5_DATA_PATH, 'r') as f:\n",
        "    actual_feature_dim = f['features'].shape[1]\n",
        "    print(f'\\n[Data] Actual feature dimension from HDF5: {actual_feature_dim}')\n",
        "\n",
        "class FeatureProjector(nn.Module):\n",
        "    \"\"\"å°†è¾¹çº§èžåˆç‰¹å¾æŠ•å½±åˆ° VQ ç©ºé—´ï¼ˆ128ç»´ï¼‰\"\"\"\n",
        "    def __init__(self, in_dim, out_dim=128):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, out_dim),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "feature_projector = FeatureProjector(in_dim=actual_feature_dim, out_dim=128).to(device)\n",
        "print(f'\\n[Projector] Feature projector: {actual_feature_dim} -> 128 dim')\n",
        "\n",
        "# ------------------ 4. æž„å»º VQ-VAE ------------------\n",
        "print('\\n[VQ-VAE] Building VQVAETransformer...')\n",
        "\n",
        "logger = logging.getLogger('end_to_end_vqvae')\n",
        "if not logger.handlers:\n",
        "    handler = logging.StreamHandler()\n",
        "    formatter = logging.Formatter('[%(levelname)s] %(message)s')\n",
        "    handler.setFormatter(formatter)\n",
        "    logger.addHandler(handler)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Decoderï¼šå°† VQ è¾“å‡ºï¼ˆ128ç»´ï¼‰é‡å»ºå›žåŽŸå§‹ç‰¹å¾ç©ºé—´\n",
        "class ReconstructionDecoder(nn.Module):\n",
        "    \"\"\"é‡å»ºè§£ç å™¨ï¼šVQ è¾“å‡º -> åŽŸå§‹ç‰¹å¾ç©ºé—´\"\"\"\n",
        "    def __init__(self, vq_dim: int, output_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(vq_dim, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, valid, true_lengths=None):\n",
        "        # x: (B, L, vq_dim)\n",
        "        return self.net(x)  # (B, L, output_dim)\n",
        "\n",
        "vq_dim = vqvae_configs.model.vqvae.vector_quantization.dim\n",
        "output_dim = actual_feature_dim  # ä½¿ç”¨å®žé™…çš„ç‰¹å¾ç»´åº¦\n",
        "\n",
        "decoder = ReconstructionDecoder(vq_dim, output_dim).to(device)\n",
        "print(f'  âœ“ Using ReconstructionDecoder: {vq_dim} -> {output_dim} dim')\n",
        "\n",
        "# ðŸ”§ ç¦ç”¨ TikTok åŽ‹ç¼©ï¼ˆé¿å…åºåˆ—é•¿åº¦å˜åŒ–ï¼‰\n",
        "if hasattr(vqvae_configs.model.vqvae.vector_quantization, 'tik_tok'):\n",
        "    vqvae_configs.model.vqvae.vector_quantization.tik_tok.enabled = False\n",
        "    print('  âš  Disabled TikTok compression to maintain sequence length')\n",
        "\n",
        "# ç¦ç”¨ kmeans_initï¼ˆè®­ç»ƒæ—¶ä¼šè‡ªåŠ¨åˆå§‹åŒ–ï¼‰\n",
        "original_kmeans_init = vqvae_configs.model.vqvae.vector_quantization.kmeans_init\n",
        "vqvae_configs.model.vqvae.vector_quantization.kmeans_init = False\n",
        "\n",
        "vqvae_model = VQVAETransformer(\n",
        "    vqvae_configs,\n",
        "    decoder=decoder,\n",
        "    logger=logger,\n",
        "    decoder_only=False\n",
        ").to(device)\n",
        "\n",
        "vqvae_configs.model.vqvae.vector_quantization.kmeans_init = original_kmeans_init\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦å¯ç”¨äº† Residual VQ\n",
        "tik_tok_cfg = vqvae_configs.model.vqvae.vector_quantization.tik_tok\n",
        "residual_depth = tik_tok_cfg.get('residual_depth', 1)\n",
        "use_residual_vq = residual_depth > 1\n",
        "\n",
        "print(f'  âœ“ VQVAETransformer built')\n",
        "print(f'    - Encoder: {vqvae_configs.model.vqvae.encoder.depth} layers, {vqvae_configs.model.vqvae.encoder.dimension} dim')\n",
        "print(f'    - VQ: {vqvae_configs.model.vqvae.vector_quantization.codebook_size} codes, {vq_dim} dim')\n",
        "print(f'    - Residual VQ: {\"enabled\" if use_residual_vq else \"disabled\"} ({residual_depth} layers)')\n",
        "print(f'    - TikTok compression: disabled (sequence length preserved)')\n",
        "print(f'    - Orthogonal regularization: weight={vqvae_configs.model.vqvae.vector_quantization.orthogonal_reg_weight}')\n",
        "\n",
        "if use_residual_vq:\n",
        "    total_codebook_combinations = vqvae_configs.model.vqvae.vector_quantization.codebook_size ** residual_depth\n",
        "    print(f'    - Total codebook combinations: {total_codebook_combinations:,} (4096^{residual_depth})')\n",
        "\n",
        "print('\\nâœ“ ç«¯åˆ°ç«¯æ¨¡åž‹æž„å»ºå®Œæˆï¼ˆä½¿ç”¨å®Œæ•´ç‰ˆ GCPNetï¼‰ï¼')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4ï¼šä¼˜åŒ–å™¨é…ç½®ï¼ˆåˆ†å±‚å­¦ä¹ çŽ‡ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Optimizer] Setting up layered learning rates...\n",
            "  âœ“ Optimizer configured with layered learning rates:\n",
            "    - GCPNet encoder: 1e-05\n",
            "    - Featuriser: 1e-05\n",
            "    - Feature projector: 0.0001\n",
            "    - VQ-VAE: 0.0001\n",
            "  âœ“ Scheduler: CosineAnnealingWarmRestarts (T_0=10, T_mult=2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_43572\\195941871.py:57: FutureWarning:\n",
            "\n",
            "`torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Mixed precision (AMP): enabled\n"
          ]
        }
      ],
      "source": [
        "print('\\n[Optimizer] Setting up layered learning rates...')\n",
        "\n",
        "# åˆ†å±‚å­¦ä¹ çŽ‡ï¼šGCPNet ç”¨å°å­¦ä¹ çŽ‡ï¼Œå…¶ä»–æ¨¡å—ç”¨æ­£å¸¸å­¦ä¹ çŽ‡\n",
        "base_lr = 1e-4\n",
        "gcpnet_lr = 1e-5  # GCPNet ç”¨ 10 å€å°çš„å­¦ä¹ çŽ‡\n",
        "\n",
        "param_groups = [\n",
        "    {\n",
        "        'params': gcpnet_encoder.parameters(),\n",
        "        'lr': gcpnet_lr,\n",
        "        'name': 'gcpnet_encoder'\n",
        "    },\n",
        "    {\n",
        "        'params': featuriser.parameters(),\n",
        "        'lr': gcpnet_lr,\n",
        "        'name': 'featuriser'\n",
        "    },\n",
        "    {\n",
        "        'params': feature_projector.parameters(),\n",
        "        'lr': base_lr,\n",
        "        'name': 'feature_projector'\n",
        "    },\n",
        "    {\n",
        "        'params': vqvae_model.parameters(),\n",
        "        'lr': base_lr,\n",
        "        'name': 'vqvae_model'\n",
        "    },\n",
        "]\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    param_groups,\n",
        "    weight_decay=0.01,\n",
        "    betas=(0.9, 0.999),\n",
        "    eps=1e-8,\n",
        ")\n",
        "\n",
        "print(f'  âœ“ Optimizer configured with layered learning rates:')\n",
        "print(f'    - GCPNet encoder: {gcpnet_lr}')\n",
        "print(f'    - Featuriser: {gcpnet_lr}')\n",
        "print(f'    - Feature projector: {base_lr}')\n",
        "print(f'    - VQ-VAE: {base_lr}')\n",
        "\n",
        "# å­¦ä¹ çŽ‡è°ƒåº¦å™¨\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "\n",
        "scheduler = CosineAnnealingWarmRestarts(\n",
        "    optimizer,\n",
        "    T_0=10,  # æ¯ 10 ä¸ª epoch é‡å¯ä¸€æ¬¡\n",
        "    T_mult=2,\n",
        "    eta_min=1e-6,\n",
        ")\n",
        "\n",
        "print(f'  âœ“ Scheduler: CosineAnnealingWarmRestarts (T_0=10, T_mult=2)')\n",
        "\n",
        "# æ··åˆç²¾åº¦è®­ç»ƒ\n",
        "use_amp = torch.cuda.is_available()\n",
        "scaler = GradScaler() if use_amp else None\n",
        "print(f'  Mixed precision (AMP): {\"enabled\" if use_amp else \"disabled\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5ï¼šæŸå¤±å‡½æ•°å®šä¹‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Loss] Loss functions defined:\n",
            "  - Reconstruction loss (MSE in feature space)\n",
            "  - VQ loss (codebook commitment, supports Residual VQ)\n",
            "  - Total loss = recon_loss + 0.1 * vq_loss\n"
          ]
        }
      ],
      "source": [
        "def compute_reconstruction_loss(pred, target, mask):\n",
        "    \"\"\"\n",
        "    è®¡ç®—é‡å»ºæŸå¤±ï¼ˆç‰¹å¾ç©ºé—´ MSEï¼‰\n",
        "    \n",
        "    Args:\n",
        "        pred: (B, L, D) é¢„æµ‹ç‰¹å¾\n",
        "        target: (B, L, D) ç›®æ ‡ç‰¹å¾\n",
        "        mask: (B, L) æœ‰æ•ˆä½ç½®æŽ©ç \n",
        "    \n",
        "    Returns:\n",
        "        loss: æ ‡é‡æŸå¤±\n",
        "    \"\"\"\n",
        "    diff = (pred - target) ** 2\n",
        "    loss = (diff * mask.unsqueeze(-1)).sum() / mask.sum().clamp(min=1)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def compute_total_loss(outputs, target_feats, mask, vq_weight=0.1):\n",
        "    \"\"\"\n",
        "    è®¡ç®—æ€»æŸå¤±ï¼ˆæ”¯æŒ Residual VQ + æ­£äº¤æ­£åˆ™åŒ–ï¼‰\n",
        "    \n",
        "    VQ Loss ç»„æˆï¼ˆæ¥è‡ª vector_quantize_pytorchï¼‰ï¼š\n",
        "    1. Commitment Loss: Î² * ||sg[z_e] - z_q||Â²\n",
        "    2. Codebook Loss: ||z_e - sg[z_q]||Â²\n",
        "    3. Orthogonal Regularization: Î»_orth * ||e_i^T e_j||_FÂ² (éžå¯¹è§’å…ƒç´ )\n",
        "    \n",
        "    æ­£äº¤æ­£åˆ™åŒ–è¯¦è§£ï¼š\n",
        "    - ç›®çš„ï¼šå¼ºåˆ¶ä¸åŒçš„ codebook å‘é‡åœ¨é«˜ç»´ç©ºé—´ä¸­åˆ†ç¦»ï¼Œé¿å…ä»£ç å†—ä½™\n",
        "    - å…¬å¼ï¼šL_orth = Î»_orth * Î£(iâ‰ j) ||e_i^T e_j||Â²\n",
        "    - æ•ˆæžœï¼šæå‡ codebook åˆ©ç”¨çŽ‡ï¼Œé˜²æ­¢ codebook collapse\n",
        "    \n",
        "    Args:\n",
        "        outputs: VQVAETransformer çš„è¾“å‡º\n",
        "        target_feats: (B, L, D) ç›®æ ‡ç‰¹å¾\n",
        "        mask: (B, L) æœ‰æ•ˆä½ç½®æŽ©ç \n",
        "        vq_weight: VQ æŸå¤±æƒé‡\n",
        "    \n",
        "    Returns:\n",
        "        total_loss: æ€»æŸå¤±\n",
        "        loss_dict: å„é¡¹æŸå¤±çš„å­—å…¸\n",
        "    \"\"\"\n",
        "    decoder_output, indices, vq_loss, ntp_logits, ntp_valid_mask, \\\n",
        "        tik_tok_padding_logits, tik_tok_padding_targets, sequence_lengths = outputs\n",
        "    \n",
        "    # é‡å»ºæŸå¤±\n",
        "    recon_loss = compute_reconstruction_loss(decoder_output, target_feats, mask)\n",
        "    \n",
        "    # VQ æŸå¤±ï¼ˆå·²åŒ…å«æ­£äº¤æ­£åˆ™åŒ–ï¼‰\n",
        "    # vq_loss çš„ç»„æˆï¼š\n",
        "    #   - commitment_loss (Î² * ||sg[z_e] - z_q||Â²)\n",
        "    #   - codebook_loss (||z_e - sg[z_q]||Â²)\n",
        "    #   - orthogonal_reg_loss (Î»_orth * orthogonal_penalty)\n",
        "    if vq_loss.dim() > 1:\n",
        "        # Residual VQ: (B, num_quantizers)\n",
        "        # æ¯å±‚çš„ vq_loss éƒ½åŒ…å«äº†æ­£äº¤æ­£åˆ™åŒ–\n",
        "        vq_loss_mean = vq_loss.sum(dim=-1).mean()\n",
        "    else:\n",
        "        # å•å±‚ VQ: (B,)\n",
        "        vq_loss_mean = vq_loss.mean()\n",
        "    \n",
        "    # æ€»æŸå¤±\n",
        "    total_loss = recon_loss + vq_weight * vq_loss_mean\n",
        "    \n",
        "    loss_dict = {\n",
        "        'total': total_loss.item(),\n",
        "        'recon': recon_loss.item(),\n",
        "        'vq': vq_loss_mean.item(),\n",
        "    }\n",
        "    \n",
        "    return total_loss, loss_dict\n",
        "\n",
        "\n",
        "print('\\n[Loss] Loss functions defined:')\n",
        "print('  - Reconstruction loss (MSE in feature space)')\n",
        "print('  - VQ loss (åŒ…å«ä»¥ä¸‹ç»„ä»¶):')\n",
        "print('    1. Commitment loss: Î² * ||sg[z_e] - z_q||Â²')\n",
        "print('    2. Codebook loss: ||z_e - sg[z_q]||Â²')\n",
        "print('    3. Orthogonal regularization: Î»_orth * ||e_i^T e_j||_FÂ²')\n",
        "print('  - Total loss = recon_loss + 0.1 * vq_loss')\n",
        "print('\\n[Config] Orthogonal regularization settings:')\n",
        "print(f'  - orthogonal_reg_weight: {vqvae_configs.model.vqvae.vector_quantization.orthogonal_reg_weight}')\n",
        "print(f'  - orthogonal_reg_max_codes: {vqvae_configs.model.vqvae.vector_quantization.orthogonal_reg_max_codes}')\n",
        "print(f'  - orthogonal_reg_active_codes_only: {vqvae_configs.model.vqvae.vector_quantization.orthogonal_reg_active_codes_only}')\n",
        "print('\\nðŸ’¡ æ­£äº¤æ­£åˆ™åŒ–çš„ä½œç”¨ï¼š')\n",
        "print('  - é˜²æ­¢ codebook collapseï¼ˆå°‘æ•° codes è¢«è¿‡åº¦ä½¿ç”¨ï¼‰')\n",
        "print('  - å¼ºåˆ¶ä¸åŒ codes åœ¨é«˜ç»´ç©ºé—´ä¸­åˆ†ç¦»ï¼Œé¿å…å†—ä½™')\n",
        "print('  - æå‡ codebook åˆ©ç”¨çŽ‡å’Œè¡¨ç¤ºå¤šæ ·æ€§')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ æ­£äº¤æ­£åˆ™åŒ–æŸå¤±è¯¦è§£\n",
        "\n",
        "### ç†è®ºèƒŒæ™¯\n",
        "\n",
        "**é—®é¢˜**ï¼šå¤§è§„æ¨¡ codebookï¼ˆå¦‚ 4096 ç»´ï¼‰å®¹æ˜“å‡ºçŽ°ï¼š\n",
        "- **ä»£ç å†—ä½™**ï¼šå¤šä¸ª codes è¡¨ç¤ºç›¸ä¼¼çš„ç‰¹å¾\n",
        "- **åˆ©ç”¨ä¸è¶³**ï¼šåªæœ‰å°‘æ•° codes è¢«é¢‘ç¹ä½¿ç”¨ï¼ˆcodebook collapseï¼‰\n",
        "\n",
        "**è§£å†³æ–¹æ¡ˆ**ï¼šæ­£äº¤æ­£åˆ™åŒ–ï¼ˆOrthogonal Regularizationï¼‰\n",
        "\n",
        "### æ•°å­¦å…¬å¼\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{orth}} = \\lambda_{\\text{orth}} \\cdot \\sum_{i \\neq j} \\|e_i^T e_j\\|_F^2\n",
        "$$\n",
        "\n",
        "å…¶ä¸­ï¼š\n",
        "- $e_i, e_j$ æ˜¯ codebook ä¸­çš„ä¸åŒå‘é‡\n",
        "- $\\|\\cdot\\|_F$ æ˜¯ Frobenius èŒƒæ•°\n",
        "- $\\lambda_{\\text{orth}}$ æ˜¯æ­£åˆ™åŒ–æƒé‡ï¼ˆé…ç½®ä¸­ä¸º 10ï¼‰\n",
        "\n",
        "### å·¥ä½œåŽŸç†\n",
        "\n",
        "1. **æƒ©ç½šç›¸å…³æ€§**ï¼šå½“ä¸¤ä¸ª codebook å‘é‡ $e_i$ å’Œ $e_j$ ç›¸ä¼¼æ—¶ï¼Œ$e_i^T e_j$ çš„å€¼å¾ˆå¤§\n",
        "2. **å¼ºåˆ¶åˆ†ç¦»**ï¼šé€šè¿‡æœ€å°åŒ– $\\|e_i^T e_j\\|^2$ï¼Œè¿«ä½¿ä¸åŒçš„ codes åœ¨é«˜ç»´ç©ºé—´ä¸­åˆ†ç¦»\n",
        "3. **æå‡å¤šæ ·æ€§**ï¼šæ¯ä¸ª code éƒ½æœ‰ç‹¬ç‰¹çš„è¡¨ç¤ºç©ºé—´ï¼Œé¿å…å†—ä½™\n",
        "\n",
        "### å®žçŽ°ç»†èŠ‚\n",
        "\n",
        "åœ¨ `vector_quantize_pytorch` ä¸­çš„å®žçŽ°ï¼š\n",
        "```python\n",
        "# åªå¯¹æ´»è·ƒçš„ codes è®¡ç®—ï¼ˆèŠ‚çœå†…å­˜ï¼‰\n",
        "orthogonal_reg_active_codes_only: True\n",
        "\n",
        "# æ¯æ¬¡åªé‡‡æ · 512 ä¸ª codes è®¡ç®—æ­£äº¤æŸå¤±ï¼ˆ4096 å¤ªå¤§ï¼‰\n",
        "orthogonal_reg_max_codes: 512\n",
        "\n",
        "# æ­£äº¤æ­£åˆ™åŒ–æƒé‡\n",
        "orthogonal_reg_weight: 10\n",
        "```\n",
        "\n",
        "### é¢„æœŸæ•ˆæžœ\n",
        "\n",
        "è®ºæ–‡ä¸­çš„æ ¸å¿ƒç»“æžœï¼š\n",
        "- âœ… **éªŒè¯é›†ä¸Š 100% codebook åˆ©ç”¨çŽ‡**\n",
        "- âœ… é¿å… codebook collapse\n",
        "- âœ… æå‡ç¦»æ•£è¡¨ç¤ºçš„å¤šæ ·æ€§"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6ï¼šè®­ç»ƒé…ç½®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Training Config]\n",
            "  Batch size: 4\n",
            "  Num epochs: 20\n",
            "  Gradient clip: 1.0\n",
            "  Total batches per epoch: 785\n",
            "  Total training steps: 15700\n",
            "  Checkpoint dir: c:\\Users\\Administrator\\Desktop\\IGEM\\stage1\\notebook-lab\\checkpoints\\vqvae_end_to_end\n"
          ]
        }
      ],
      "source": [
        "# è®­ç»ƒè¶…å‚æ•°\n",
        "BATCH_SIZE = 4  # ç«¯åˆ°ç«¯è®­ç»ƒæ˜¾å­˜å ç”¨å¤§ï¼Œå‡å° batch size\n",
        "NUM_EPOCHS = 20\n",
        "GRADIENT_CLIP = 1.0\n",
        "LOG_INTERVAL = 10  # æ¯ 10 ä¸ª batch æ‰“å°ä¸€æ¬¡\n",
        "SAVE_INTERVAL = 5  # æ¯ 5 ä¸ª epoch ä¿å­˜ä¸€æ¬¡\n",
        "\n",
        "# æ•°æ®åŠ è½½å™¨\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0,  # Windows ä¸Šå»ºè®®è®¾ä¸º 0\n",
        "    pin_memory=True if torch.cuda.is_available() else False,\n",
        ")\n",
        "\n",
        "print('\\n[Training Config]')\n",
        "print(f'  Batch size: {BATCH_SIZE}')\n",
        "print(f'  Num epochs: {NUM_EPOCHS}')\n",
        "print(f'  Gradient clip: {GRADIENT_CLIP}')\n",
        "print(f'  Total batches per epoch: {len(train_loader)}')\n",
        "print(f'  Total training steps: {len(train_loader) * NUM_EPOCHS}')\n",
        "print(f'  Checkpoint dir: {CHECKPOINT_DIR}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7ï¼šç«¯åˆ°ç«¯è®­ç»ƒå¾ªçŽ¯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "å¼€å§‹ç«¯åˆ°ç«¯è®­ç»ƒ\n",
            "======================================================================\n",
            "\n",
            "[Debug] Testing one batch to check dimensions...\n",
            "  edge_feats shape: torch.Size([4, 512, 641])\n",
            "  projected_feats shape: torch.Size([4, 512, 128])\n",
            "  decoder_output shape: torch.Size([4, 512, 641])\n",
            "  Expected: decoder_output should be torch.Size([4, 512, 641])\n",
            "\n",
            "âœ… ç»´åº¦åŒ¹é…ï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚\n",
            "\n",
            "======================================================================\n",
            "å¼€å§‹æ­£å¼è®­ç»ƒ\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Epoch 1/20\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_43572\\3963581821.py:69: FutureWarning:\n",
            "\n",
            "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Batch [10/785] Loss: 2.3400 (Recon: 1.3095, VQ: 10.3046) LR: 1.00e-05\n",
            "  Batch [20/785] Loss: 2.6009 (Recon: 1.6169, VQ: 9.8402) LR: 1.00e-05\n",
            "  Batch [30/785] Loss: 3.5331 (Recon: 2.3803, VQ: 11.5274) LR: 1.00e-05\n",
            "  Batch [40/785] Loss: 3.1462 (Recon: 2.0923, VQ: 10.5390) LR: 1.00e-05\n",
            "  Batch [50/785] Loss: 3.5842 (Recon: 2.5810, VQ: 10.0326) LR: 1.00e-05\n",
            "  Batch [60/785] Loss: 2.2716 (Recon: 1.2695, VQ: 10.0217) LR: 1.00e-05\n",
            "  Batch [70/785] Loss: 4.1137 (Recon: 3.1099, VQ: 10.0379) LR: 1.00e-05\n",
            "  Batch [80/785] Loss: 3.2617 (Recon: 2.2520, VQ: 10.0978) LR: 1.00e-05\n",
            "  Batch [90/785] Loss: 2.5940 (Recon: 1.5838, VQ: 10.1024) LR: 1.00e-05\n",
            "  Batch [100/785] Loss: 2.8287 (Recon: 1.7984, VQ: 10.3026) LR: 1.00e-05\n",
            "  Batch [110/785] Loss: 2.8551 (Recon: 1.8565, VQ: 9.9856) LR: 1.00e-05\n",
            "  Batch [120/785] Loss: 4.3086 (Recon: 3.0718, VQ: 12.3689) LR: 9.99e-06\n",
            "  Batch [130/785] Loss: 3.2638 (Recon: 2.1713, VQ: 10.9251) LR: 9.99e-06\n",
            "  Batch [140/785] Loss: 3.1683 (Recon: 2.1009, VQ: 10.6739) LR: 9.99e-06\n",
            "  Batch [150/785] Loss: 2.9343 (Recon: 1.8580, VQ: 10.7624) LR: 9.99e-06\n",
            "  Batch [160/785] Loss: 2.3810 (Recon: 1.3842, VQ: 9.9671) LR: 9.99e-06\n",
            "  Batch [170/785] Loss: 3.5087 (Recon: 2.2449, VQ: 12.6372) LR: 9.99e-06\n",
            "  Batch [180/785] Loss: 1.5640 (Recon: 0.5513, VQ: 10.1271) LR: 9.99e-06\n",
            "  Batch [190/785] Loss: 2.2855 (Recon: 1.2363, VQ: 10.4922) LR: 9.99e-06\n",
            "  Batch [200/785] Loss: 2.8048 (Recon: 1.7660, VQ: 10.3883) LR: 9.99e-06\n",
            "  Batch [210/785] Loss: 3.5227 (Recon: 2.5101, VQ: 10.1257) LR: 9.98e-06\n",
            "  Batch [220/785] Loss: 1.8469 (Recon: 0.8531, VQ: 9.9379) LR: 9.98e-06\n",
            "  Batch [230/785] Loss: 3.0296 (Recon: 1.9892, VQ: 10.4045) LR: 9.98e-06\n",
            "  Batch [240/785] Loss: 2.7996 (Recon: 1.7971, VQ: 10.0250) LR: 9.98e-06\n",
            "  Batch [250/785] Loss: 2.1732 (Recon: 1.1559, VQ: 10.1732) LR: 9.98e-06\n",
            "  Batch [260/785] Loss: 4.3136 (Recon: 3.2809, VQ: 10.3271) LR: 9.98e-06\n",
            "  Batch [270/785] Loss: 3.4917 (Recon: 2.4758, VQ: 10.1583) LR: 9.97e-06\n",
            "  Batch [280/785] Loss: 2.8449 (Recon: 1.7618, VQ: 10.8311) LR: 9.97e-06\n",
            "  Batch [290/785] Loss: 2.9718 (Recon: 1.8929, VQ: 10.7894) LR: 9.97e-06\n",
            "  Batch [300/785] Loss: 1.6158 (Recon: 0.6551, VQ: 9.6069) LR: 9.97e-06\n",
            "  Batch [310/785] Loss: 2.8287 (Recon: 1.8176, VQ: 10.1109) LR: 9.97e-06\n",
            "  Batch [320/785] Loss: 2.0641 (Recon: 1.0764, VQ: 9.8775) LR: 9.96e-06\n",
            "  Batch [330/785] Loss: 3.0421 (Recon: 1.8059, VQ: 12.3613) LR: 9.96e-06\n",
            "  Batch [340/785] Loss: 4.2724 (Recon: 3.2301, VQ: 10.4222) LR: 9.96e-06\n",
            "  Batch [350/785] Loss: 3.7784 (Recon: 2.7085, VQ: 10.6991) LR: 9.96e-06\n",
            "  Batch [360/785] Loss: 2.8862 (Recon: 1.8298, VQ: 10.5641) LR: 9.95e-06\n",
            "  Batch [370/785] Loss: 6.0505 (Recon: 5.0584, VQ: 9.9214) LR: 9.95e-06\n",
            "  Batch [380/785] Loss: 3.0952 (Recon: 2.1009, VQ: 9.9426) LR: 9.95e-06\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     92\u001b[39m     scaler.unscale_(optimizer)\n\u001b[32m     93\u001b[39m     torch.nn.utils.clip_grad_norm_(\n\u001b[32m     94\u001b[39m         [p \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m param_groups \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m]],\n\u001b[32m     95\u001b[39m         GRADIENT_CLIP\n\u001b[32m     96\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     scaler.update()\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# ç‰¹å¾æŠ•å½±\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\amp\\grad_scaler.py:465\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    459\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    462\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    463\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\amp\\grad_scaler.py:359\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    352\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    353\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    356\u001b[39m     **kwargs: Any,\n\u001b[32m    357\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    358\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf_per_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    360\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\amp\\grad_scaler.py:359\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    352\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    353\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    356\u001b[39m     **kwargs: Any,\n\u001b[32m    357\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    358\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    360\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "print('\\n' + '=' * 70)\n",
        "print('å¼€å§‹ç«¯åˆ°ç«¯è®­ç»ƒ')\n",
        "print('=' * 70)\n",
        "\n",
        "# è®¾ç½®æ‰€æœ‰æ¨¡åž‹ä¸ºè®­ç»ƒæ¨¡å¼\n",
        "gcpnet_encoder.train()\n",
        "featuriser.train()\n",
        "feature_projector.train()\n",
        "vqvae_model.train()\n",
        "\n",
        "# è®­ç»ƒåŽ†å²\n",
        "train_history = {\n",
        "    'epoch': [],\n",
        "    'total_loss': [],\n",
        "    'recon_loss': [],\n",
        "    'vq_loss': [],\n",
        "    'codebook_usage': [],  # æ–°å¢žï¼šcodebook ä½¿ç”¨çŽ‡\n",
        "}\n",
        "\n",
        "# å…ˆæµ‹è¯•ä¸€ä¸ª batch çœ‹ç»´åº¦\n",
        "print('\\n[Debug] Testing one batch to check dimensions...')\n",
        "test_batch = next(iter(train_loader))\n",
        "edge_feats, mask = test_batch\n",
        "edge_feats = edge_feats.to(device)\n",
        "mask = mask.to(device)\n",
        "nan_mask = torch.ones_like(mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    projected_feats = feature_projector(edge_feats)\n",
        "    outputs = vqvae_model(projected_feats, mask, nan_mask)\n",
        "    decoder_output = outputs[0]\n",
        "    \n",
        "    print(f'  edge_feats shape: {edge_feats.shape}')\n",
        "    print(f'  projected_feats shape: {projected_feats.shape}')\n",
        "    print(f'  decoder_output shape: {decoder_output.shape}')\n",
        "    print(f'  Expected: decoder_output should be {edge_feats.shape}')\n",
        "    \n",
        "    if decoder_output.shape != edge_feats.shape:\n",
        "        print(f'\\nâŒ ç»´åº¦ä¸åŒ¹é…ï¼')\n",
        "        print(f'   Decoder è¾“å‡º: {decoder_output.shape}')\n",
        "        print(f'   æœŸæœ›è¾“å‡º: {edge_feats.shape}')\n",
        "        print(f'\\nè¯·æ£€æŸ¥ Cell 10 ä¸­çš„ ReconstructionDecoder æ˜¯å¦æ­£ç¡®æž„å»ºã€‚')\n",
        "        raise RuntimeError('Decoder output dimension mismatch')\n",
        "    else:\n",
        "        print(f'\\nâœ… ç»´åº¦åŒ¹é…ï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚')\n",
        "\n",
        "print('\\n' + '=' * 70)\n",
        "print('å¼€å§‹æ­£å¼è®­ç»ƒ')\n",
        "print('=' * 70)\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    print(f'\\n{\"=\" * 70}')\n",
        "    print(f'Epoch {epoch}/{NUM_EPOCHS}')\n",
        "    print(f'{\"=\" * 70}')\n",
        "    \n",
        "    epoch_losses = {'total': 0.0, 'recon': 0.0, 'vq': 0.0}\n",
        "    epoch_codes = []  # æ”¶é›†æ‰€æœ‰ codes ç”¨äºŽç»Ÿè®¡\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (edge_feats, mask) in enumerate(train_loader):\n",
        "        edge_feats = edge_feats.to(device)  # (B, L, 641)\n",
        "        mask = mask.to(device)  # (B, L)\n",
        "        nan_mask = torch.ones_like(mask)  # å‡è®¾æ²¡æœ‰ NaN\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # å‰å‘ä¼ æ’­ï¼ˆç«¯åˆ°ç«¯ï¼Œæ¢¯åº¦ä¼šå›žä¼ åˆ° GCPNetï¼‰\n",
        "        if use_amp:\n",
        "            with autocast():\n",
        "                # ç‰¹å¾æŠ•å½±\n",
        "                projected_feats = feature_projector(edge_feats)  # (B, L, 128)\n",
        "                \n",
        "                # VQ-VAE å‰å‘\n",
        "                outputs = vqvae_model(projected_feats, mask, nan_mask)\n",
        "                indices = outputs[1]  # èŽ·å– codebook indices\n",
        "                \n",
        "                # æ”¶é›† codesï¼ˆç”¨äºŽç»Ÿè®¡ codebook ä½¿ç”¨çŽ‡ï¼‰\n",
        "                if indices.dim() == 3:\n",
        "                    # Residual VQ: (B, L, num_quantizers)\n",
        "                    epoch_codes.append(indices[:, :, 0].flatten().cpu())  # åªç»Ÿè®¡ç¬¬ä¸€å±‚\n",
        "                else:\n",
        "                    # å•å±‚ VQ: (B, L)\n",
        "                    epoch_codes.append(indices.flatten().cpu())\n",
        "                \n",
        "                # è®¡ç®—æŸå¤±ï¼ˆä½¿ç”¨åŽŸå§‹ edge_feats ä½œä¸ºé‡å»ºç›®æ ‡ï¼‰\n",
        "                total_loss, loss_dict = compute_total_loss(\n",
        "                    outputs, edge_feats, mask, vq_weight=0.1\n",
        "                )\n",
        "            \n",
        "            # åå‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰\n",
        "            scaler.scale(total_loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for group in param_groups for p in group['params']],\n",
        "                GRADIENT_CLIP\n",
        "            )\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # ç‰¹å¾æŠ•å½±\n",
        "            projected_feats = feature_projector(edge_feats)\n",
        "            \n",
        "            # VQ-VAE å‰å‘\n",
        "            outputs = vqvae_model(projected_feats, mask, nan_mask)\n",
        "            indices = outputs[1]\n",
        "            \n",
        "            # æ”¶é›† codes\n",
        "            if indices.dim() == 3:\n",
        "                epoch_codes.append(indices[:, :, 0].flatten().cpu())\n",
        "            else:\n",
        "                epoch_codes.append(indices.flatten().cpu())\n",
        "            \n",
        "            # è®¡ç®—æŸå¤±ï¼ˆä½¿ç”¨åŽŸå§‹ edge_feats ä½œä¸ºé‡å»ºç›®æ ‡ï¼‰\n",
        "            total_loss, loss_dict = compute_total_loss(\n",
        "                outputs, edge_feats, mask, vq_weight=0.1\n",
        "            )\n",
        "            \n",
        "            # åå‘ä¼ æ’­\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for group in param_groups for p in group['params']],\n",
        "                GRADIENT_CLIP\n",
        "            )\n",
        "            optimizer.step()\n",
        "        \n",
        "        scheduler.step(epoch - 1 + batch_idx / len(train_loader))\n",
        "        \n",
        "        # ç´¯ç§¯æŸå¤±\n",
        "        for key in epoch_losses:\n",
        "            epoch_losses[key] += loss_dict[key]\n",
        "        num_batches += 1\n",
        "        \n",
        "        # å®šæœŸæ‰“å°\n",
        "        if (batch_idx + 1) % LOG_INTERVAL == 0:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f'  Batch [{batch_idx+1}/{len(train_loader)}] '\n",
        "                  f'Loss: {loss_dict[\"total\"]:.4f} '\n",
        "                  f'(Recon: {loss_dict[\"recon\"]:.4f}, VQ: {loss_dict[\"vq\"]:.4f}) '\n",
        "                  f'LR: {current_lr:.2e}')\n",
        "    \n",
        "    # Epoch ç»Ÿè®¡\n",
        "    avg_losses = {k: v / num_batches for k, v in epoch_losses.items()}\n",
        "    \n",
        "    # è®¡ç®— codebook ä½¿ç”¨çŽ‡\n",
        "    all_codes = torch.cat(epoch_codes)\n",
        "    unique_codes = torch.unique(all_codes)\n",
        "    codebook_usage = len(unique_codes) / vqvae_configs.model.vqvae.vector_quantization.codebook_size\n",
        "    \n",
        "    print(f'\\n{\"=\" * 70}')\n",
        "    print(f'Epoch {epoch} Summary:')\n",
        "    print(f'  Avg Total Loss: {avg_losses[\"total\"]:.4f}')\n",
        "    print(f'  Avg Recon Loss: {avg_losses[\"recon\"]:.4f}')\n",
        "    print(f'  Avg VQ Loss: {avg_losses[\"vq\"]:.4f}')\n",
        "    print(f'  Codebook Usage: {len(unique_codes)}/{vqvae_configs.model.vqvae.vector_quantization.codebook_size} ({codebook_usage*100:.2f}%)')\n",
        "    print(f'{\"=\" * 70}')\n",
        "    \n",
        "    # è®°å½•åŽ†å²\n",
        "    train_history['epoch'].append(epoch)\n",
        "    train_history['total_loss'].append(avg_losses['total'])\n",
        "    train_history['recon_loss'].append(avg_losses['recon'])\n",
        "    train_history['vq_loss'].append(avg_losses['vq'])\n",
        "    train_history['codebook_usage'].append(codebook_usage)\n",
        "    \n",
        "    # ä¿å­˜ checkpoint\n",
        "    if epoch % SAVE_INTERVAL == 0 or epoch == NUM_EPOCHS:\n",
        "        checkpoint_path = CHECKPOINT_DIR / f'epoch_{epoch}.pth'\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'gcpnet_encoder_state_dict': gcpnet_encoder.state_dict(),\n",
        "            'featuriser_state_dict': featuriser.state_dict(),\n",
        "            'feature_projector_state_dict': feature_projector.state_dict(),\n",
        "            'vqvae_model_state_dict': vqvae_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_history': train_history,\n",
        "            'config': {\n",
        "                'vqvae': OmegaConf.to_container(vqvae_configs),\n",
        "                'gcpnet': OmegaConf.to_container(gcpnet_configs),\n",
        "            },\n",
        "        }, checkpoint_path)\n",
        "        print(f'\\n  âœ“ Saved checkpoint to {checkpoint_path}')\n",
        "\n",
        "print('\\n' + '=' * 70)\n",
        "print('âœ“ ç«¯åˆ°ç«¯è®­ç»ƒå®Œæˆï¼')\n",
        "print('=' * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8ï¼šè®­ç»ƒåŽ†å²å¯è§†åŒ–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x400 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ Training curves saved to c:\\Users\\Administrator\\Desktop\\IGEM\\stage1\\notebook-lab\\checkpoints\\vqvae_end_to_end\\training_curves.png\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Total loss\n",
        "axes[0, 0].plot(train_history['epoch'], train_history['total_loss'], 'b-', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Total Loss')\n",
        "axes[0, 0].set_title('Total Loss')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Reconstruction loss\n",
        "axes[0, 1].plot(train_history['epoch'], train_history['recon_loss'], 'g-', linewidth=2)\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Reconstruction Loss')\n",
        "axes[0, 1].set_title('Reconstruction Loss')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# VQ loss\n",
        "axes[1, 0].plot(train_history['epoch'], train_history['vq_loss'], 'r-', linewidth=2)\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('VQ Loss')\n",
        "axes[1, 0].set_title('VQ Loss (âš ï¸ ä¸Šå‡è¡¨ç¤º Codebook Collapse)')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Codebook usage\n",
        "axes[1, 1].plot(train_history['epoch'], \n",
        "                [usage * 100 for usage in train_history['codebook_usage']], \n",
        "                'purple', linewidth=2, marker='o')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Codebook Usage (%)')\n",
        "axes[1, 1].set_title('Codebook Usage (åº”è¯¥ä¿æŒé«˜ä½)')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].axhline(y=50, color='orange', linestyle='--', label='50% threshold')\n",
        "axes[1, 1].axhline(y=25, color='red', linestyle='--', label='25% threshold (å±é™©)')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(CHECKPOINT_DIR / 'training_curves_with_codebook.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nâœ“ Training curves saved to {CHECKPOINT_DIR / \"training_curves_with_codebook.png\"}')\n",
        "\n",
        "# æ‰“å°è¯Šæ–­ä¿¡æ¯\n",
        "print('\\n' + '=' * 70)\n",
        "print('Codebook Collapse è¯Šæ–­')\n",
        "print('=' * 70)\n",
        "final_usage = train_history['codebook_usage'][-1] * 100\n",
        "if final_usage < 25:\n",
        "    print(f'âš ï¸ ä¸¥é‡ Codebook Collapseï¼åªä½¿ç”¨äº† {final_usage:.1f}% çš„ codes')\n",
        "    print('å»ºè®®ï¼š')\n",
        "    print('  1. å¢žå¤§ commitment_weightï¼ˆå½“å‰ 0.25 â†’ 0.5ï¼‰')\n",
        "    print('  2. å¯ç”¨ kmeans_init')\n",
        "    print('  3. å‡å° VQ æƒé‡ï¼ˆå½“å‰ 0.1 â†’ 0.05ï¼‰')\n",
        "elif final_usage < 50:\n",
        "    print(f'âš ï¸ ä¸­åº¦ Codebook Collapseï¼Œä½¿ç”¨äº† {final_usage:.1f}% çš„ codes')\n",
        "    print('å»ºè®®ï¼šè°ƒæ•´ commitment_weight æˆ– VQ æƒé‡')\n",
        "else:\n",
        "    print(f'âœ… Codebook ä½¿ç”¨æ­£å¸¸ï¼Œä½¿ç”¨äº† {final_usage:.1f}% çš„ codes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9ï¼šæŽ¨ç†éªŒè¯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print('\\n[Codebook Analysis] åˆ†æž codebook çš„æ­£äº¤æ€§...')\n",
        "\n",
        "# èŽ·å– codebook å‘é‡\n",
        "with torch.no_grad():\n",
        "    # è®¿é—® VQ layer çš„ codebook\n",
        "    if hasattr(vqvae_model, 'vector_quantizer'):\n",
        "        vq_layer = vqvae_model.vector_quantizer\n",
        "        \n",
        "        # èŽ·å– codebook embeddings\n",
        "        if hasattr(vq_layer, 'codebook'):\n",
        "            codebook = vq_layer.codebook  # (codebook_size, dim)\n",
        "        elif hasattr(vq_layer, '_codebook'):\n",
        "            codebook = vq_layer._codebook.embed\n",
        "        else:\n",
        "            print('âš ï¸ æ— æ³•è®¿é—® codebook')\n",
        "            codebook = None\n",
        "        \n",
        "        if codebook is not None:\n",
        "            codebook_np = codebook.cpu().numpy()\n",
        "            \n",
        "            # 1. è®¡ç®— codebook å‘é‡çš„ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆé‡‡æ ·å‰ 100 ä¸ª codesï¼‰\n",
        "            n_sample = min(100, codebook_np.shape[0])\n",
        "            codebook_sample = codebook_np[:n_sample]\n",
        "            \n",
        "            # å½’ä¸€åŒ–\n",
        "            codebook_norm = codebook_sample / (np.linalg.norm(codebook_sample, axis=1, keepdims=True) + 1e-8)\n",
        "            \n",
        "            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰\n",
        "            similarity_matrix = np.dot(codebook_norm, codebook_norm.T)\n",
        "            \n",
        "            # 2. å¯è§†åŒ–\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "            \n",
        "            # 2.1 ç›¸ä¼¼åº¦çŸ©é˜µçƒ­å›¾\n",
        "            sns.heatmap(similarity_matrix, cmap='coolwarm', center=0, \n",
        "                       vmin=-1, vmax=1, ax=axes[0], cbar_kws={'label': 'Cosine Similarity'})\n",
        "            axes[0].set_title(f'Codebook Similarity Matrix (å‰ {n_sample} codes)')\n",
        "            axes[0].set_xlabel('Code Index')\n",
        "            axes[0].set_ylabel('Code Index')\n",
        "            \n",
        "            # 2.2 éžå¯¹è§’å…ƒç´ çš„åˆ†å¸ƒï¼ˆæ­£äº¤æ€§æŒ‡æ ‡ï¼‰\n",
        "            off_diagonal = similarity_matrix[~np.eye(n_sample, dtype=bool)]\n",
        "            axes[1].hist(off_diagonal, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
        "            axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Perfect Orthogonal')\n",
        "            axes[1].set_xlabel('Cosine Similarity')\n",
        "            axes[1].set_ylabel('Frequency')\n",
        "            axes[1].set_title('éžå¯¹è§’å…ƒç´ åˆ†å¸ƒï¼ˆè¶ŠæŽ¥è¿‘ 0 è¶Šæ­£äº¤ï¼‰')\n",
        "            axes[1].legend()\n",
        "            axes[1].grid(True, alpha=0.3)\n",
        "            \n",
        "            # ç»Ÿè®¡ä¿¡æ¯\n",
        "            mean_sim = np.abs(off_diagonal).mean()\n",
        "            max_sim = np.abs(off_diagonal).max()\n",
        "            axes[1].text(0.05, 0.95, \n",
        "                        f'Mean |similarity|: {mean_sim:.4f}\\nMax |similarity|: {max_sim:.4f}',\n",
        "                        transform=axes[1].transAxes, \n",
        "                        verticalalignment='top',\n",
        "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "            \n",
        "            # 2.3 Codebook å‘é‡çš„èŒƒæ•°åˆ†å¸ƒ\n",
        "            norms = np.linalg.norm(codebook_np, axis=1)\n",
        "            axes[2].hist(norms, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
        "            axes[2].set_xlabel('L2 Norm')\n",
        "            axes[2].set_ylabel('Frequency')\n",
        "            axes[2].set_title('Codebook å‘é‡èŒƒæ•°åˆ†å¸ƒ')\n",
        "            axes[2].grid(True, alpha=0.3)\n",
        "            \n",
        "            # ç»Ÿè®¡ä¿¡æ¯\n",
        "            mean_norm = norms.mean()\n",
        "            std_norm = norms.std()\n",
        "            axes[2].text(0.05, 0.95, \n",
        "                        f'Mean norm: {mean_norm:.4f}\\nStd norm: {std_norm:.4f}',\n",
        "                        transform=axes[2].transAxes, \n",
        "                        verticalalignment='top',\n",
        "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(CHECKPOINT_DIR / 'codebook_orthogonality.png', dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            \n",
        "            print(f'\\nâœ“ Codebook åˆ†æžå®Œæˆ')\n",
        "            print(f'  - Codebook size: {codebook_np.shape[0]}')\n",
        "            print(f'  - Codebook dim: {codebook_np.shape[1]}')\n",
        "            print(f'  - Mean |similarity| (éžå¯¹è§’): {mean_sim:.4f} (è¶Šå°è¶Šå¥½ï¼Œç†æƒ³å€¼æŽ¥è¿‘ 0)')\n",
        "            print(f'  - Max |similarity| (éžå¯¹è§’): {max_sim:.4f}')\n",
        "            print(f'  - Mean norm: {mean_norm:.4f}')\n",
        "            \n",
        "            # æ­£äº¤æ€§è¯„åˆ†\n",
        "            if mean_sim < 0.1:\n",
        "                print(f'\\nâœ… æ­£äº¤æ€§ä¼˜ç§€ï¼Codebook å‘é‡é«˜åº¦åˆ†ç¦»')\n",
        "            elif mean_sim < 0.3:\n",
        "                print(f'\\nâœ… æ­£äº¤æ€§è‰¯å¥½')\n",
        "            elif mean_sim < 0.5:\n",
        "                print(f'\\nâš ï¸ æ­£äº¤æ€§ä¸€èˆ¬ï¼Œå»ºè®®å¢žå¤§ orthogonal_reg_weight')\n",
        "            else:\n",
        "                print(f'\\nâŒ æ­£äº¤æ€§è¾ƒå·®ï¼Œå­˜åœ¨ä¸¥é‡çš„ä»£ç å†—ä½™')\n",
        "                print(f'   å»ºè®®ï¼šå¢žå¤§ orthogonal_reg_weight (å½“å‰ 10 â†’ 20)')\n",
        "    else:\n",
        "        print('âš ï¸ æ¨¡åž‹ä¸­æ²¡æœ‰ vector_quantizer å±žæ€§')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n[Inference] Testing trained model...')\n",
        "\n",
        "# è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
        "gcpnet_encoder.eval()\n",
        "featuriser.eval()\n",
        "feature_projector.eval()\n",
        "vqvae_model.eval()\n",
        "\n",
        "# éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬\n",
        "test_idx = np.random.randint(0, len(dataset))\n",
        "edge_feats, mask = dataset[test_idx]\n",
        "edge_feats = edge_feats.unsqueeze(0).to(device)  # (1, L, 641)\n",
        "mask = mask.unsqueeze(0).to(device)  # (1, L)\n",
        "nan_mask = torch.ones_like(mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # æŠ•å½±\n",
        "    projected_feats = feature_projector(edge_feats)  # (1, L, 128)\n",
        "    \n",
        "    # ç¼–ç \n",
        "    outputs = vqvae_model(projected_feats, mask, nan_mask)\n",
        "    decoder_output, indices, vq_loss, _, _, _, _, _ = outputs\n",
        "    \n",
        "    # è®¡ç®—é‡å»ºè¯¯å·®\n",
        "    valid_len = mask[0].sum().item()\n",
        "    original = edge_feats[0, :valid_len].cpu().numpy()\n",
        "    reconstructed = decoder_output[0, :valid_len].cpu().numpy()\n",
        "    \n",
        "    mse = ((original - reconstructed) ** 2).mean()\n",
        "    \n",
        "    # æ£€æŸ¥æ˜¯å¦æ˜¯ Residual VQ\n",
        "    if indices.dim() == 3:\n",
        "        # Residual VQ: (B, L, num_quantizers)\n",
        "        codes = indices[0, :valid_len].cpu().numpy()\n",
        "        print(f'\\nTest sample {test_idx}:')\n",
        "        print(f'  Valid edges: {valid_len}')\n",
        "        print(f'  Reconstruction MSE: {mse:.6f}')\n",
        "        print(f'  Residual VQ depth: {codes.shape[1]}')\n",
        "        \n",
        "        # ç»Ÿè®¡æ¯å±‚çš„ unique codes\n",
        "        for layer in range(codes.shape[1]):\n",
        "            layer_codes = codes[:, layer]\n",
        "            unique_codes = len(np.unique(layer_codes))\n",
        "            print(f'  Layer {layer} unique codes: {unique_codes} / 4096')\n",
        "            print(f'  Layer {layer} codes sample: {layer_codes[:10]}')\n",
        "    else:\n",
        "        # å•å±‚ VQ: (B, L)\n",
        "        codes = indices[0, :valid_len].cpu().numpy()\n",
        "        unique_codes = len(np.unique(codes))\n",
        "        print(f'\\nTest sample {test_idx}:')\n",
        "        print(f'  Valid edges: {valid_len}')\n",
        "        print(f'  Reconstruction MSE: {mse:.6f}')\n",
        "        print(f'  Unique codes used: {unique_codes} / 4096')\n",
        "        print(f'  Codes sample: {codes[:10]}')\n",
        "\n",
        "print('\\nâœ“ æŽ¨ç†éªŒè¯å®Œæˆï¼')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10ï¼šå¯¼å‡º edge codesï¼ˆå¯é€‰ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n[Export] Encoding all edges to codes...')\n",
        "\n",
        "# è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
        "gcpnet_encoder.eval()\n",
        "featuriser.eval()\n",
        "feature_projector.eval()\n",
        "vqvae_model.eval()\n",
        "\n",
        "all_codes = []\n",
        "all_meta = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx in range(len(dataset)):\n",
        "        if (idx + 1) % 100 == 0:\n",
        "            print(f'  Progress: {idx+1}/{len(dataset)}', end='\\r')\n",
        "        \n",
        "        edge_feats, mask = dataset[idx]\n",
        "        edge_feats = edge_feats.unsqueeze(0).to(device)\n",
        "        mask = mask.unsqueeze(0).to(device)\n",
        "        nan_mask = torch.ones_like(mask)\n",
        "        \n",
        "        # ç¼–ç \n",
        "        projected_feats = feature_projector(edge_feats)\n",
        "        outputs = vqvae_model(projected_feats, mask, nan_mask)\n",
        "        _, indices, _, _, _, _, _, _ = outputs\n",
        "        \n",
        "        # æå–æœ‰æ•ˆ codes\n",
        "        valid_len = mask[0].sum().item()\n",
        "        codes = indices[0, :valid_len].cpu().numpy()\n",
        "        \n",
        "        all_codes.extend(codes.tolist())\n",
        "        all_meta.extend([idx] * len(codes))\n",
        "\n",
        "print(f'\\n  Total codes: {len(all_codes)}')\n",
        "\n",
        "# ä¿å­˜ä¸º CSV\n",
        "codes_df = pd.DataFrame({\n",
        "    'graph_index': all_meta,\n",
        "    'edge_code': all_codes,\n",
        "})\n",
        "\n",
        "output_csv = BASE_DIR / 'binding_edge_codes_end_to_end.csv'\n",
        "codes_df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f'\\nâœ“ Edge codes saved to {output_csv}')\n",
        "print(f'  Unique codes: {codes_df[\"edge_code\"].nunique()} / 4096')\n",
        "print(f'  Code usage: {codes_df[\"edge_code\"].nunique() / 4096 * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ€»ç»“\n",
        "\n",
        "### ç«¯åˆ°ç«¯è®­ç»ƒ vs å†»ç»“ GCPNet\n",
        "\n",
        "| é¡¹ç›® | å†»ç»“ GCPNet | ç«¯åˆ°ç«¯è®­ç»ƒ |\n",
        "|------|------------|----------|\n",
        "| **è®­ç»ƒé€Ÿåº¦** | å¿« | æ…¢ï¼ˆçº¦ 2-3 å€ï¼‰ |\n",
        "| **æ˜¾å­˜å ç”¨** | ä½Ž | é«˜ï¼ˆéœ€å‡å° batch sizeï¼‰ |\n",
        "| **æ”¶æ•›é€Ÿåº¦** | å¿« | æ…¢ï¼ˆéœ€æ›´å¤š epochï¼‰ |\n",
        "| **æœ€ç»ˆæ•ˆæžœ** | ä¾èµ–é¢„è®­ç»ƒè´¨é‡ | ç†è®ºä¸Šæ›´å¥½ï¼ˆè”åˆä¼˜åŒ–ï¼‰ |\n",
        "| **é€‚ç”¨åœºæ™¯** | é¢„è®­ç»ƒç‰¹å¾å·²è¶³å¤Ÿå¥½ | éœ€è¦é’ˆå¯¹ä»»åŠ¡å¾®è°ƒç‰¹å¾ |\n",
        "\n",
        "### å»ºè®®\n",
        "\n",
        "1. **å…ˆå†»ç»“è®­ç»ƒ**ï¼šç”¨ `binding_edge_codebook.ipynb` å¿«é€ŸéªŒè¯æµç¨‹\n",
        "2. **å†ç«¯åˆ°ç«¯å¾®è°ƒ**ï¼šå¦‚æžœæ•ˆæžœä¸ç†æƒ³ï¼Œä½¿ç”¨æœ¬ notebook è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–\n",
        "3. **ç›‘æŽ§è¿‡æ‹Ÿåˆ**ï¼šç«¯åˆ°ç«¯è®­ç»ƒæ›´å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå»ºè®®åŠ å…¥éªŒè¯é›†\n",
        "4. **è°ƒæ•´è¶…å‚æ•°**ï¼š\n",
        "   - GCPNet å­¦ä¹ çŽ‡ï¼š1e-6 ~ 1e-5\n",
        "   - å…¶ä»–æ¨¡å—å­¦ä¹ çŽ‡ï¼š1e-4 ~ 1e-3\n",
        "   - Batch sizeï¼š2-4ï¼ˆæ ¹æ®æ˜¾å­˜ï¼‰\n",
        "   - Gradient clipï¼š0.5-1.0\n",
        "\n",
        "### ä¸‹ä¸€æ­¥\n",
        "\n",
        "- æ·»åŠ éªŒè¯é›†è¯„ä¼°\n",
        "- å®žçŽ° early stopping\n",
        "- å¯¹æ¯”å†»ç»“ vs ç«¯åˆ°ç«¯çš„æ•ˆæžœ\n",
        "- å¯è§†åŒ–å­¦åˆ°çš„ codes åˆ†å¸ƒ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
